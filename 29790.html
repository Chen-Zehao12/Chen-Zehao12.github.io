<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>pytorch | 陈泽豪</title><meta name="author" content="陈泽豪"><meta name="copyright" content="陈泽豪"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1. Pytorch 是什么  他是一个基于 Python 的科学计算包，目标用户有两类  为了使用 GPU 来替代 numpy 一个深度学习援救平台：提供最大的灵活性和速度    1.1 开始   张量（Tensors）   张量类似于 numpy 的 ndarrays，不同之处在于张量可以使用 GPU 来加快计算 12from __future__ import print_functioni">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch">
<meta property="og:url" content="https://chenzehao.com/29790.html">
<meta property="og:site_name" content="陈泽豪">
<meta property="og:description" content="1. Pytorch 是什么  他是一个基于 Python 的科学计算包，目标用户有两类  为了使用 GPU 来替代 numpy 一个深度学习援救平台：提供最大的灵活性和速度    1.1 开始   张量（Tensors）   张量类似于 numpy 的 ndarrays，不同之处在于张量可以使用 GPU 来加快计算 12from __future__ import print_functioni">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">
<meta property="article:published_time" content="2020-07-17T16:00:00.000Z">
<meta property="article:modified_time" content="2022-01-21T05:48:55.890Z">
<meta property="article:author" content="陈泽豪">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><link rel="shortcut icon" href="https://tva1.sinaimg.cn/large/008i3skNgy1gyk8f9yfnqj305k05kglg.jpg"><link rel="canonical" href="https://chenzehao.com/29790"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><meta name="baidu-site-verification" content="code-GhGfwpqKHn"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?9191e58322566f7fdcbb360b8eeb1686";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'pytorch',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-01-21 13:48:55'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.0.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gyk8f9yfnqj305k05kglg.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">91</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">29</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><span> 笔记</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">陈泽豪</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><span> 笔记</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">pytorch</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-07-17T16:00:00.000Z" title="发表于 2020-07-18 00:00:00">2020-07-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-01-21T05:48:55.890Z" title="更新于 2022-01-21 13:48:55">2022-01-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>22分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="1-Pytorch 是什么">1. Pytorch 是什么</h2>
<ul>
<li>他是一个基于 Python 的科学计算包，目标用户有两类
<ul>
<li>为了使用 GPU 来替代 numpy</li>
<li>一个深度学习援救平台：提供最大的灵活性和速度</li>
</ul>
</li>
</ul>
<h3 id="1-1- 开始">1.1 开始</h3>
<ul>
<li>
<p><strong>张量（Tensors）</strong></p>
</li>
<li>
<p>张量类似于 numpy 的 ndarrays，不同之处在于张量可以使用 GPU 来加快计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>构建一个未初始化的 5*3 的矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[8.9082e-39, 1.0194e-38, 9.1837e-39],</span></span><br><span class="line"><span class="string">        [4.6837e-39, 9.9184e-39, 9.0000e-39],</span></span><br><span class="line"><span class="string">        [1.0561e-38, 1.0653e-38, 4.1327e-39],</span></span><br><span class="line"><span class="string">        [8.9082e-39, 9.8265e-39, 9.4592e-39],</span></span><br><span class="line"><span class="string">        [1.0561e-38, 1.0653e-38, 1.0469e-38]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>构建一个随机初始化的矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.9299, 0.4400, 0.3392],</span></span><br><span class="line"><span class="string">        [0.3961, 0.5855, 0.0385],</span></span><br><span class="line"><span class="string">        [0.1097, 0.0796, 0.4999],</span></span><br><span class="line"><span class="string">        [0.6187, 0.4093, 0.9464],</span></span><br><span class="line"><span class="string">        [0.2045, 0.0679, 0.8407]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>获取矩阵的大小</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.size())  <span class="comment"># torch.Size([5, 3])</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>注意</p>
<ul>
<li>torch.Size 实际上是一个元组，所以它支持元组相同的操作</li>
</ul>
</li>
</ul>
<h3 id="1-2- 操作">1.2 操作</h3>
<ul>
<li>
<p>张量上的操作有多重语法形式，下面我们以加法为例进行讲解</p>
</li>
<li>
<p>语法 1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x + y)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[1.3904, 1.0933, 1.4198],</span></span><br><span class="line"><span class="string">        [1.3160, 0.1825, 0.7505],</span></span><br><span class="line"><span class="string">        [1.6783, 1.1296, 1.0064],</span></span><br><span class="line"><span class="string">        [1.4538, 0.4579, 1.2522],</span></span><br><span class="line"><span class="string">        [0.6313, 0.6540, 0.3648]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>语法 2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.add(x, y))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[1.5780, 1.3004, 0.9931],</span></span><br><span class="line"><span class="string">        [0.5637, 1.1872, 1.2551],</span></span><br><span class="line"><span class="string">        [0.3259, 0.8970, 1.8662],</span></span><br><span class="line"><span class="string">        [1.3873, 0.5955, 0.8541],</span></span><br><span class="line"><span class="string">        [1.2324, 1.7460, 0.6771]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>语法 3：给出一个输出向量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">result = torch.Tensor(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[1.7647, 1.1190, 0.5791],</span></span><br><span class="line"><span class="string">        [1.0025, 1.2801, 1.1568],</span></span><br><span class="line"><span class="string">        [0.8231, 1.7458, 0.9347],</span></span><br><span class="line"><span class="string">        [1.0131, 1.4638, 0.9090],</span></span><br><span class="line"><span class="string">        [1.1472, 1.3238, 1.0470]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>语法 4：原地操作（in-place）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">y.add_(x)  <span class="comment"># 把 x 加到 y 上</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.8214, 0.4328, 1.1097],</span></span><br><span class="line"><span class="string">        [0.9083, 1.0851, 0.3501],</span></span><br><span class="line"><span class="string">        [0.1495, 1.2647, 0.8343],</span></span><br><span class="line"><span class="string">        [0.9466, 1.1204, 1.3263],</span></span><br><span class="line"><span class="string">        [0.7314, 0.9936, 0.2871]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>注意
<ul>
<li>任何在原地 (in-place) 改变张量的操作都有一个’_' 后缀</li>
</ul>
</li>
</ul>
</li>
<li>
<p>可以使用所有的 numpy 索引操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x[:, <span class="number">1</span>])  <span class="comment"># tensor([0.5275, 0.7537, 0.8832, 0.2474, 0.7361])</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="1-3-numpy 桥">1.3 numpy 桥</h3>
<ul>
<li>
<p>把一个 torch 张量转换为 numpy 数组或者反过来都是很简单的</p>
</li>
<li>
<p>Torch 张量和 numpy 数组将共享潜在的内存，改变其中一个也将改变另一个</p>
</li>
<li>
<p>把 Torch 张量转换为 numpy 数组</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">b = a.numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(b))  <span class="comment"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>把 numpy 数组转换为 Torch 张量
<ul>
<li>所有在 CPU 上的张量，除了字符张量，都支持在 numpy 之间转换</li>
</ul>
</li>
</ul>
<h3 id="1-4-CUDA 张量">1.4 CUDA 张量</h3>
<ul>
<li>
<p>使用.cuda 函数可以将张量移动到 GPU 上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># let us run this cell only if CUDA is available</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    x = x.cuda()</span><br><span class="line">    y = y.cuda()</span><br><span class="line">    x + y</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="2-Autograd- 自动求导 -automatic-differentiation">2. Autograd: 自动求导(automatic differentiation)</h2>
<ul>
<li>
<p>PyTorch 中所有神经网络的核心是 <strong>autograd</strong> 包。我们首先简单介绍一下这个包，然后训练我们的第一个神经网络。</p>
</li>
<li>
<p><strong>autograd</strong>提供了自动求导。它是一个运行时定义的框架，这意味着反向传播是根据你的代码如何运行来定义，并且每次迭代可以不同</p>
</li>
<li>
<p>接下来我们用一些简单的示例来看这个包</p>
</li>
</ul>
<h3 id="2-1- 变量 -Variable">2.1 变量(Variable)</h3>
<ul>
<li>
<p>autograd.Variable 是 autograd 包的核心类。它包装了张量，支持几乎所有的张量上的操作。一旦你完成你的前向计算，可以通过.backward()方法来自动计算所有的梯度</p>
</li>
<li>
<p>你可以通过.data 属性来访问变量中的原始张量，关于这个变量的梯度被计算放入.grad 属性中</p>
</li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/5208761-4d002b98f0558b2e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/184/format/webp" alt="img"></p>
<ul>
<li>
<p>对自动求导的实现还有一个非常重要的类，即函数(Function)</p>
</li>
<li>
<p>变量 (Variable) 和函数 (Function) 是相互联系的，并形成一个非循环图来构建一个完整的计算过程。每个变量有一个.grad_fn 属性，它指向创建该变量的一个 Function，用户自己创建的变量除外，它的 grad_fn 属性为 None</p>
</li>
<li>
<p>如果你想计算导数, 可以在一个变量上调用.backward()。如果一个 Variable 是一个标量(它只有一个元素值)，你不必给该方法指定任何的参数，但是该 Variable 有多个值，你需要指定一个和该变量相同形状的 grad_output 参数(查看 API 发现实际为 gradients 参数)</p>
</li>
<li>
<p>创建一个变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">x = Variable(torch.ones(<span class="number">2</span>, <span class="number">2</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1.]], requires_grad=True)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>在变量上执行操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y = x + <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[3., 3.],</span></span><br><span class="line"><span class="string">        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>因为 y 是通过一个操作创建的，所以它有 grad_fn，而 x 是由用户创建，所以它的 grad_fn 为 None</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(y.grad_fn)    <span class="comment"># &lt;AddBackward0 object at 0x0000019C4237C308&gt;</span></span><br><span class="line"><span class="built_in">print</span>(x.grad_fn)    <span class="comment"># None</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>在 y 上执行操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(z, out)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[27., 27.],</span></span><br><span class="line"><span class="string">        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-2- 梯度 -Gradients">2.2 梯度(Gradients)</h3>
<ul>
<li>
<p>现在我们来执行反向传播，out.backward()相当于执行 out.backward(torch.Tensor([1.0]))</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出 out 对 x 的梯度 d(out)/dx</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[4.5000, 4.5000],</span></span><br><span class="line"><span class="string">        [4.5000, 4.5000]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>你应该得到一个值全为 4.5 的矩阵，我们把变量 out 称为 o，则</p>
<p><img src="https://upload-images.jianshu.io/upload_images/5208761-448b1f5ea9349f5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/290/format/webp" alt="img"></p>
<p>因此</p>
<p><img src="https://upload-images.jianshu.io/upload_images/5208761-85d7832d4fa1a1fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/216/format/webp" alt="img"></p>
</li>
<li>
<p>我们还可以用自动求导做更多有趣的事</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>)</span><br><span class="line">x = Variable(x, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y)    <span class="comment"># tensor([1166.5090,  711.3173, 1023.9789], grad_fn=&lt;MulBackward0&gt;)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="3- 神经网络">3. 神经网络</h2>
<ul>
<li>
<p>可以使用 torch.nn 包来构建神经网络</p>
</li>
<li>
<p>你已知道 autograd 包，nn 包依赖 autograd 包来定义模型并求导。一个 nn.Module 包含各个层和一个 forward(input)方法，该方法返回 output</p>
</li>
<li>
<p>例如，我们来看一下下面这个分类数字图像的网络</p>
<p><img src="https://upload-images.jianshu.io/upload_images/5208761-18cb30cbde34d75d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/759/format/webp" alt="img"></p>
</li>
<li>
<p>他是一个简单的前馈神经网络，它接受一个输入，然后一层接着一层的输入，直到最后得到结果</p>
</li>
<li>
<p>神经网络的典型训练过程如下：</p>
<ol>
<li>
<p>定义神经网络模型，它有一些可学习的参数(或者权重)</p>
</li>
<li>
<p>在数据集上迭代</p>
</li>
<li>
<p>通过神经网络处理输入</p>
</li>
<li>
<p>计算损失(输出结果和正确值的差距大小)</p>
</li>
<li>
<p>将梯度反向传播回网络的参数</p>
</li>
<li>
<p>更新网络的参数，主要使用如下简单的更新原则</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weight = weight - learning_rate * gradient</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ul>
<h3 id="3-1- 定义网络">3.1 定义网络</h3>
<ul>
<li>
<p>我们先定义一个网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 继承原有模型</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        <span class="comment"># 定义了两个卷积层</span></span><br><span class="line">        <span class="comment"># 第一层是输入 1 维的（说明是单通道，灰色的图片）图片，输出 6 维的的卷积层（说明用到了 6 个卷积核，而每个卷积核是 5*5 的）。</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 第一层是输入 1 维的（说明是单通道，灰色的图片）图片，输出 6 维的的卷积层（说明用到了 6 个卷积核，而每个卷积核是 5*5 的）。</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        <span class="comment"># 定义了三个全连接层，即 fc1 与 conv2 相连，将 16 张 5*5 的卷积网络一维化，并输出 120 个节点。</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        <span class="comment"># 将 120 个节点转化为 84 个。</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        <span class="comment"># 将 84 个节点输出为 10 个，即有 10 个分类结果。</span></span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        <span class="comment"># 用 relu 激活函数作为一个池化层，池化的窗口大小是 2*2，这个也与上文的 16*5*5 的计算结果相符（一开始我没弄懂为什么 fc1 的输入点数是 16*5*5, 后来发现，这个例子是建立在 lenet5 上的）。</span></span><br><span class="line">        <span class="comment"># 这句整体的意思是，先用 conv1 卷积，然后激活，激活的窗口是 2*2。</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        <span class="comment"># 作用同上，然后有个需要注意的地方是在窗口是正方形的时候，2 的写法等同于（2，2）。</span></span><br><span class="line">        <span class="comment"># 这句整体的意思是，先用 conv2 卷积，然后激活，激活的窗口是 2*2。</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 这句整体的意思是，调用下面的定义好的查看特征数量的函数，将我们高维的向量转化为一维。</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        <span class="comment"># 用一下全连接层 fc1，然后做一个激活。</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        <span class="comment"># 用一下全连接层 fc2，然后做一个激活。</span></span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        <span class="comment"># 用一下全连接层 fc3。</span></span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 承接上文的引用，这里需要注意的是，由于 pytorch 只接受图片集的输入方式（原文的单词是 batch）, 所以第一个代表个数的维度被忽略。</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Net(</span></span><br><span class="line"><span class="string">  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="string">  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="string">  (fc1): Linear(in_features=400, out_features=120, bias=True)</span></span><br><span class="line"><span class="string">  (fc2): Linear(in_features=120, out_features=84, bias=True)</span></span><br><span class="line"><span class="string">  (fc3): Linear(in_features=84, out_features=10, bias=True)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>你只需定义 forward 函数，backward 函数 (计算梯度) 在使用 autograd 时自动为你创建。你可以在 forward 函数中使用 Tensor 的任何操作</p>
</li>
<li>
<p>net.parameters()返回模型需要学习的参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">list</span>(net.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(params))      <span class="comment"># 10</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    <span class="built_in">print</span>(param.size())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([6, 1, 5, 5])</span></span><br><span class="line"><span class="string">torch.Size([6])</span></span><br><span class="line"><span class="string">torch.Size([16, 6, 5, 5])</span></span><br><span class="line"><span class="string">torch.Size([16])</span></span><br><span class="line"><span class="string">torch.Size([120, 400])</span></span><br><span class="line"><span class="string">torch.Size([120])</span></span><br><span class="line"><span class="string">torch.Size([84, 120])</span></span><br><span class="line"><span class="string">torch.Size([84])</span></span><br><span class="line"><span class="string">torch.Size([10, 84])</span></span><br><span class="line"><span class="string">torch.Size([10])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>forward 的输入和输出都是 autograd.Variable</p>
<ul>
<li>注意：这个网络 (LeNet) 期望的输入大小是 32*32. 如果使用 MNIST 数据集来训练这个网络，请把图片大小重新调整到 32*32</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = Variable(torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[-0.0112,  0.0500,  0.0875, -0.0451,  0.0880, -0.0892,  0.1751, -0.0456,</span></span><br><span class="line"><span class="string">          0.0007,  0.1839]], grad_fn=&lt;AddmmBackward&gt;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>将所有参数的梯度缓存清零，然后进行随机梯度的的反向传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>注意</p>
<ol>
<li>torch.nn 只支持小批量输入，整个 torch.nn 包都只支持小批量样本，而不支持单个样本</li>
<li>例如，nn.Conv2d 将接受一个 4 维的张量，每一维分别是 sSamples * nChannels * Height * Width(样本数 * 通道数 * 高 * 宽)</li>
<li>如果你有单个样本，只需使用 input.unsqueeze(0)来添加其它的维数</li>
</ol>
</li>
<li>
<p>回顾</p>
<ul>
<li>
<p>torch.Tensor：一个多维数组</p>
</li>
<li>
<p>autograd.Variable：包装一个 Tensor，记录在其上执行过的操作。除了拥有 Tensor 拥有的 API，还有类似 backward()的 API，也保存关于这个向量的梯度</p>
</li>
<li>
<p>nn.Module：神经网络模块。封装参数，移动到 GPU 上运行，导出，加载等</p>
</li>
<li>
<p>nn.Parameter：一种变量，当把它赋值给一个 Module 时，被自动的注册为一个参数</p>
</li>
<li>
<p>autograd.Function：实现一个自动求导操作的前向和反向定义, 每个变量操作至少创建一个函数节点</p>
</li>
</ul>
</li>
</ul>
<h3 id="3-2- 损失函数">3.2 损失函数</h3>
<ul>
<li>
<p>先介绍一下损失函数是干什么的：它可以用来度量输出和目标之间的差距，那度量出来有什么意义呢？还记得我们的反向传播吗？他可以将误差作为一个反馈来影响我们之前的参数，更新参数将会在下一节中讲到。当然度量的方法有很多，我们这里选用 nn.MSELoss 来计算误差，下面接着完善上面的例程</p>
</li>
<li>
<p>一个损失函数接受一对 (output, target) 作为输入(output 为网络的输出，target 为实际值)，计算一个值来估计网络的输出和目标值相差多少</p>
</li>
<li>
<p>在 nn 包中有几种不同的损失函数。一个简单的损失函数是：nn.MSELoss，他计算输入 (个人认为是网络的输出) 和目标值之间的均方误差</p>
</li>
<li>
<p>例如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这一部分是来搞定损失函数</span></span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line">target = Variable(torch.arange(<span class="number">1</span>, <span class="number">11</span>))  <span class="comment"># a dummy target, for example</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(out, target)</span><br><span class="line"><span class="built_in">print</span>(loss)  <span class="comment"># tensor(38.4099, grad_fn=&lt;MseLossBackward&gt;)</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>现在，你反向跟踪 loss，使用它的.grad_fn 属性，你会看到向下面这样的一个计算图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>所以，当你调用 loss.backward()，整个图关于损失被求导，图中所有变量将拥有.grad 变量来累计他们的梯度</p>
</li>
<li>
<p>为了说明，我们反向跟踪几步</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看一看我们的各个点的结果</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn)  <span class="comment"># MSELoss</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># Linear</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># ReLU</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;MseLossBackward object at 0x000001AE892A1988&gt;</span></span><br><span class="line"><span class="string">&lt;AddmmBackward object at 0x000001AE853ECD88&gt;</span></span><br><span class="line"><span class="string">&lt;AccumulateGrad object at 0x000001AE892A1988&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="3-3- 反向传播">3.3 反向传播</h3>
<ul>
<li>
<p>为了反向传播误差，我们所需做的是调用 loss.backward()。你需要清除已存在的梯度，否则梯度将被累加到已存在的梯度。</p>
</li>
<li>
<p>现在，我们将调用 loss.backward()，并查看 conv1 层的偏置项在反向传播前后的梯度</p>
</li>
<li>
<p>输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重点来了，反向传播计算梯度。</span></span><br><span class="line">net.zero_grad()     <span class="comment"># zeroes the gradient buffers of all parameters</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad before backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad after backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">conv1.bias.grad before backward</span></span><br><span class="line"><span class="string">tensor([0., 0., 0., 0., 0., 0.])</span></span><br><span class="line"><span class="string">conv1.bias.grad after backward</span></span><br><span class="line"><span class="string">tensor([0.0087, -0.0073,  0.0013,  0.0006, -0.0107, -0.0042])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="3-4- 更新权重">3.4 更新权重</h3>
<ul>
<li>
<p>实践中最简单的更新规则是随机梯度下降(SGD)</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>=</mo><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>−</mo><mi>l</mi><mi>e</mi><mi>a</mi><mi>r</mi><mi>n</mi><mi>i</mi><mi>n</mi><msub><mi>g</mi><mi>r</mi></msub><mi>a</mi><mi>t</mi><mi>e</mi><mo>∗</mo><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">weight = weight - learning_rate * gradient
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">e</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">e</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">nin</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mord mathnormal">i</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span></span></span></span></span></p>
</li>
<li>
<p>我们可以使用简单的 Python 代码实现这个规则</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 相应的 python 代码</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>然而，当你使用神经网络是，你想要使用各种不同的更新规则，比如 SGD，Nesterov-SGD，Adam，RMSPROP 等。为了能做到这一点，我们构建了一个包 torch.optim 实现了所有的这些规则。使用他们非常简单</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># create your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in your trainning loop:</span></span><br><span class="line">optimizer.zero_grad()  <span class="comment"># zero the gradient buffers</span></span><br><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">loss = criter(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.setp()  <span class="comment"># does the update</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="4- 训练一个分类器">4. 训练一个分类器</h2>
<ul>
<li>我们将一次按照下列顺序进行
<ol>
<li>使用 torchvision 加载和归一化 CIFAR10 训练集和测试集</li>
<li>定义一个卷积神经网络 CNN</li>
<li>定义损失函数</li>
<li>在训练集上训练网络</li>
<li>在测试集上测试网络</li>
</ol>
</li>
</ul>
<h3 id="4-1- 加载和归一化 CIFAR10">4.1 加载和归一化 CIFAR10</h3>
<ul>
<li>
<p>使用 torchvision 加载 CIFAR10 是非常容易的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>torchvision 的输出是 [0, 1] 的 PILImage 图像，我们把它转换为归一化范围为 [-1, 1] 的张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([transforms.ToTensor(),</span><br><span class="line">                                transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, </span><br><span class="line">                                        download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                          shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                       download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                         shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">输出</span></span><br><span class="line"><span class="string">Files already downloaded and verified</span></span><br><span class="line"><span class="string">Files already downloaded and verified</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>为了好玩，我们展示一些训练图像</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># functions to show an image</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span>(<span class="params">img</span>):</span></span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get some random training images</span></span><br><span class="line">dataiter = <span class="built_in">iter</span>(trainloader)</span><br><span class="line">images, labels = dataiter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment"># print labels</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">truck   cat   car plane</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src="https://upload-images.jianshu.io/upload_images/5208761-13f9e99f6296072a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640/format/webp" alt="img"></p>
</li>
</ul>
<h3 id="4-2- 定义一个卷积神经网络">4.2 定义一个卷积神经网络</h3>
<ul>
<li>从之前的神经网络一节复制神经网络代码，并修改为接受 3 通道图像取代之前的接受单通道图像</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)	 <span class="comment"># 修改为接受 3 通道图像</span></span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>
<h3 id="4-3- 定义损失函数和优化器">4.3 定义损失函数和优化器</h3>
<ul>
<li>
<p>我们使用交叉熵作为损失函数，使用带动量的随机梯度下降</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="4-4- 训练网络">4.4 训练网络</h3>
<ul>
<li>
<p>这是开始有趣的时刻。我们只需在数据迭代器上循环，数据输入给网络，并优化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># wrap them in Variable</span></span><br><span class="line">        inputs, labels = Variable(inputs), Variable(labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.data</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[1,  2000] loss: 2.191</span></span><br><span class="line"><span class="string">[1,  4000] loss: 1.866</span></span><br><span class="line"><span class="string">[1,  6000] loss: 1.696</span></span><br><span class="line"><span class="string">[1,  8000] loss: 1.596</span></span><br><span class="line"><span class="string">[1, 10000] loss: 1.502</span></span><br><span class="line"><span class="string">[1, 12000] loss: 1.496</span></span><br><span class="line"><span class="string">[2,  2000] loss: 1.422</span></span><br><span class="line"><span class="string">[2,  4000] loss: 1.370</span></span><br><span class="line"><span class="string">[2,  6000] loss: 1.359</span></span><br><span class="line"><span class="string">[2,  8000] loss: 1.321</span></span><br><span class="line"><span class="string">[2, 10000] loss: 1.311</span></span><br><span class="line"><span class="string">[2, 12000] loss: 1.275</span></span><br><span class="line"><span class="string">Finished Training</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="4-5- 在测试集上测试网络">4.5 在测试集上测试网络</h3>
<ul>
<li>
<p>我们在整个训练集上训练了两次网络，但是我们需要检查网络是否从数据集中学习到东西</p>
</li>
<li>
<p>我们通过预测神经网络输出的类别标签并根据实际情况进行检测。如果预测正确，我们把该样本添加到正确预测列表</p>
</li>
<li>
<p>第一步，显示测试集中的图片一遍熟悉图片内容</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dataiter = <span class="built_in">iter</span>(testloader)</span><br><span class="line">images, labels = dataiter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;GroundTruth: &#x27;</span>, <span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">GroundTruth:    cat  ship  ship plane</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src="https://upload-images.jianshu.io/upload_images/5208761-a2de1eb289881a2d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640/format/webp" alt="img"></p>
</li>
<li>
<p>现在我们来看看神经网络认为以上图片是什么</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outputs = net(Variable(images))</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出是 10 个标签的能量。一个类别的能量越大，神经网络越认为他是这个类别。所以让我们得到最高能量的标签</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">_, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predicted: &#x27;</span>, <span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[predicted[j]]</span><br><span class="line">                              <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Predicted:    cat  ship   car plane</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>这结果看起来非常的好</p>
</li>
<li>
<p>接下来让我们看看网络在整个测试集上的结果如何</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">    images, labels = data</span><br><span class="line">    outputs = net(Variable(images))</span><br><span class="line">    _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">    total += labels.size(<span class="number">0</span>)</span><br><span class="line">    correct += (predicted == labels).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy of the network on the 10000 test images: %d %%&#x27;</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Accuracy of the network on the 10000 test images: 55 %</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>结果看起来好于偶然，偶然的正确率为 10%，似乎网络学习到了一些东西</p>
</li>
<li>
<p>那在什么类上预测较好，什么类预测结果不好呢</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">class_correct = <span class="built_in">list</span>(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>))</span><br><span class="line">class_total = <span class="built_in">list</span>(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">    images, labels = data</span><br><span class="line">    outputs = net(Variable(images))</span><br><span class="line">    _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">    c = (predicted == labels).squeeze()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        label = labels[i]</span><br><span class="line">        class_correct[label] += c[i]</span><br><span class="line">        class_total[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy of %5s : %2d %%&#x27;</span> % (</span><br><span class="line">        classes[i], <span class="number">100</span> * class_correct[i] / class_total[i]))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Accuracy of plane : 60 %</span></span><br><span class="line"><span class="string">Accuracy of   car : 46 %</span></span><br><span class="line"><span class="string">Accuracy of  bird : 44 %</span></span><br><span class="line"><span class="string">Accuracy of   cat : 35 %</span></span><br><span class="line"><span class="string">Accuracy of  deer : 38 %</span></span><br><span class="line"><span class="string">Accuracy of   dog : 43 %</span></span><br><span class="line"><span class="string">Accuracy of  frog : 57 %</span></span><br><span class="line"><span class="string">Accuracy of horse : 76 %</span></span><br><span class="line"><span class="string">Accuracy of  ship : 71 %</span></span><br><span class="line"><span class="string">Accuracy of truck : 74 %</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="4-6- 在 GPU 上训练">4.6 在 GPU 上训练</h3>
<ul>
<li>
<p>你是如何把一个 Tensor 转换 GPU 上，你就如何把一个神经网络移动到 GPU 上训练。这个操作会递归遍历有所模块，并将其参数和缓冲区转换为 CUDA 张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.cuda()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>请记住，你也必须在每一步中把你的输入和目标值转换到 GPU 上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs, labels = Variable(inputs.cuda()), Variable(target.cuda())</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>为什么我们没注意到 GPU 的速度提升很多？那是因为网络非常的小</p>
</li>
<li>
<p>实践</p>
<ul>
<li>尝试增加你的网络的宽度(第一个 nn.Conv2d 的第 2 个参数，第二个 nn.Conv2d 的第一个参数，他们需要是相同的数字)，看看你得到了什么样的加速</li>
</ul>
</li>
</ul>
<h2 id="5-CUDA 安装">5. CUDA 安装</h2>
<h3 id="5-1-CUDA-10-0">5.1 CUDA 10.0</h3>
<ul>
<li>链接：<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-10.0-download-archive?target_os=Windows&amp;target_arch=x86_64&amp;target_version=10&amp;target_type=exelocal">https://developer.nvidia.com/cuda-10.0-download-archive?target_os=Windows&amp;target_arch=x86_64&amp;target_version=10&amp;target_type=exelocal</a></li>
</ul>
<p><img src="https://s1.ax1x.com/2020/07/18/UgjDXV.png" alt="image"></p>
<ul>
<li>安装 CUDA 时选择自定义安装，并将 Visual Studio Integration 一项去掉打勾，否则有可能安装出错</li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/18605189-3fc452b02609fe39.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/596/format/webp" alt="img"></p>
<ul>
<li>检验安装是否安装正确：win+R→cmd→nvcc -V</li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/18605189-e2f10ae8a18bd317.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/976/format/webp" alt="img"></p>
<h3 id="5-2-cuDNN-7-4-1">5.2 cuDNN 7.4.1</h3>
<ul>
<li>链接：<a target="_blank" rel="noopener" href="https://developer.nvidia.com/rdp/cudnn-archive">https://developer.nvidia.com/rdp/cudnn-archive</a></li>
</ul>
<p><img src="https://s1.ax1x.com/2020/07/18/UgvJjx.png" alt="image"></p>
<ul>
<li>将 cuDNN 解压后的文件夹中 bin、include 和 lib 三个文件夹复制粘贴到 CUDA 安装路径中这三个文件夹的父文件夹下面，Windows 中默认安装路径为：C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0</li>
</ul>
<h3 id="5-3-gpu 版 pytorch">5.3 gpu 版 pytorch</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision cudatoolkit=<span class="number">10.0</span> -c pytorch</span><br></pre></td></tr></table></figure></article><div class="tag_share"><div class="post_share"><div class="social-share" data-image="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Pytorch%20%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-text">1. Pytorch 是什么</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%20%E5%BC%80%E5%A7%8B"><span class="toc-text">1.1 开始</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%20%E6%93%8D%E4%BD%9C"><span class="toc-text">1.2 操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-numpy%20%E6%A1%A5"><span class="toc-text">1.3 numpy 桥</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-CUDA%20%E5%BC%A0%E9%87%8F"><span class="toc-text">1.4 CUDA 张量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Autograd-%20%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%20-automatic-differentiation"><span class="toc-text">2. Autograd: 自动求导(automatic differentiation)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%20%E5%8F%98%E9%87%8F%20-Variable"><span class="toc-text">2.1 变量(Variable)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%20%E6%A2%AF%E5%BA%A6%20-Gradients"><span class="toc-text">2.2 梯度(Gradients)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">3. 神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%20%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C"><span class="toc-text">3.1 定义网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">3.2 损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">3.3 反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%20%E6%9B%B4%E6%96%B0%E6%9D%83%E9%87%8D"><span class="toc-text">3.4 更新权重</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%20%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-text">4. 训练一个分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%20%E5%8A%A0%E8%BD%BD%E5%92%8C%E5%BD%92%E4%B8%80%E5%8C%96%20CIFAR10"><span class="toc-text">4.1 加载和归一化 CIFAR10</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%20%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%AA%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">4.2 定义一个卷积神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%20%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">4.3 定义损失函数和优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%20%E8%AE%AD%E7%BB%83%E7%BD%91%E7%BB%9C"><span class="toc-text">4.4 训练网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%20%E5%9C%A8%E6%B5%8B%E8%AF%95%E9%9B%86%E4%B8%8A%E6%B5%8B%E8%AF%95%E7%BD%91%E7%BB%9C"><span class="toc-text">4.5 在测试集上测试网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%20%E5%9C%A8%20GPU%20%E4%B8%8A%E8%AE%AD%E7%BB%83"><span class="toc-text">4.6 在 GPU 上训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-CUDA%20%E5%AE%89%E8%A3%85"><span class="toc-text">5. CUDA 安装</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-CUDA-10-0"><span class="toc-text">5.1 CUDA 10.0</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-cuDNN-7-4-1"><span class="toc-text">5.2 cuDNN 7.4.1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-gpu%20%E7%89%88%20pytorch"><span class="toc-text">5.3 gpu 版 pytorch</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2022 By 陈泽豪</div><div class="footer_custom_text">我是豪豪</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script></div></body></html>